<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bengali.AI Handwritten Grapheme Classification - Midway Blog | Cheng  Zeng</title>
    <meta name="author" content="Cheng  Zeng">
    <meta name="description" content="Classify the components of handwritten Bengali">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="stylesheet" href="/website/assets/css/main.css">
    <link rel="canonical" href="https://cengc13.github.io/website/kaggle/2020/02/25/kaggle-bengali-midway.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/website/assets/js/theme.js"></script>
    <script src="/website/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/website/"><span class="font-weight-bold">Cheng </span>Zeng</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/website/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/website/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/website/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/website/projects/">Projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-3">
            <nav id="toc-sidebar" class="sticky-top"></nav>
          </div>
          <!-- main content area -->
          <div class="col-sm-9">
            <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Bengali.AI Handwritten Grapheme Classification - Midway Blog</h1>
    <p class="post-meta">February 25, 2020</p>
    <p class="post-tags">
      <a href="/website/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>
        ·  
        <a href="/website/blog/tag/computer-vision">
          <i class="fas fa-hashtag fa-sm"></i> computer-vision</a>  
          <a href="/website/blog/tag/data-science">
          <i class="fas fa-hashtag fa-sm"></i> data-science</a>  
          <a href="/website/blog/tag/multiclass-classification">
          <i class="fas fa-hashtag fa-sm"></i> multiclass-classification</a>  
          
        ·  
        <a href="/website/blog/category/kaggle">
          <i class="fas fa-tag fa-sm"></i> kaggle</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <!-- En-Dash         &ndash;    &#150;
Em-Dash         &mdash;    &#151;
Minus Symbol    &minus;    &#8722; -->

<p><strong>Team: Zzz…</strong></p>

<p><strong>Members: Cheng Zeng, Zhi Wang, Peter Huang</strong></p>

<p><strong>Comparing different CNN architectures</strong></p>

<p>We compared the performance using the basic CNN model, <code class="language-plaintext highlighter-rouge">Densenet121</code> and <code class="language-plaintext highlighter-rouge">Densenet169</code>, <code class="language-plaintext highlighter-rouge">Resnet</code> and <code class="language-plaintext highlighter-rouge">Efficientnet</code>. <code class="language-plaintext highlighter-rouge">Basic CNN model</code> and <code class="language-plaintext highlighter-rouge">Densenet</code> can give reasonable training accuracy while for <code class="language-plaintext highlighter-rouge">Resnet</code> and <code class="language-plaintext highlighter-rouge">Efficient</code>, it is not easy to find a local minimum (training is not stable). We finally choose <code class="language-plaintext highlighter-rouge">Densenet121</code> since its training converges steadily and it gives good accuracy. Note that although <code class="language-plaintext highlighter-rouge">Densenet169</code> is denser and has more parameters, we found significant overfitting with this model.</p>

<h2 id="overview-of-processed-dataset">Overview of processed dataset</h2>
<p>Before we go into details of the CNN model used in this competition, we look at some basic info of the preprocessed dataset. Each image is now of 64\(\times\)64\(\times\)1 size, and the entire dataset has been split to training and validation datasets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">IMG_SIZE</span><span class="o">=</span><span class="mi">64</span>
<span class="n">N_CHANNELS</span><span class="o">=</span><span class="mi">1</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Training images: </span><span class="si">{</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Training labels root: </span><span class="si">{</span><span class="n">Y_train_root</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Training labels vowel: </span><span class="si">{</span><span class="n">Y_train_vowel</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Training labels consonants: </span><span class="si">{</span><span class="n">Y_train_consonant</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="img-div" style="text-align:center">
  <figure>

  <picture>
    

    <!-- Fallback to the original file -->
   <img src="/website/assets/img/kaggle-bengali/midway-blog/dataset-summary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Summary of processed training images" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

  <figcaption>A summary of processed training data</figcaption>
</div>

<h2 id="densenet121-model">Densenet121 model</h2>

<p>Densenet contains a feature layer (convolutional layer) capturing low-level features from images, several dense blocks, and transition layers between adjacent dense blocks.</p>

<h3 id="dense-block">Dense block</h3>

<p>To reduce the computation, a 1\(\times\)1 convolutional layer (bottleneck layer) is added, which makes the second convolutional layer always has a fixed input depth. It is also easy to see the size (width and height) of the feature maps keeps the same through the dense layer, which makes it easy to stack any number of dense layers together to build a dense block. For example, densenet121 has four dense blocks, which have 6, 12, 24, 16 dense layers.</p>

<h3 id="transition-layer">Transition layer</h3>

<p>As a tradition, the size of the output of every layer in CNN decreases in order to abstract higher-level features. In densenet, the transition layers take this responsibility while the dense blocks keep the size and depth. Every transition layer contains a 1\(\times\)1 convolutional layer and a 2\(\times\)2 average pooling layer with a stride of 2 to reduce the size to the half. Be aware that transition layers also receive all the output from all the layers of its last dense block. So the 1\(\times\)1 convolutional layer reduces the depth to a fixed number, while the average pooling reduces the size.</p>

<div class="img-div" style="text-align:center">
  <figure>

  <picture>
    

    <!-- Fallback to the original file -->
   <img src="/website/assets/img/kaggle-bengali/midway-blog/densenet-topology.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Densenet layer-by-layer structure" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

  <figcaption>Densenet121 layer topology</figcaption>
</div>

<div class="img-div" style="text-align:center">
  <figure>

  <picture>
    

    <!-- Fallback to the original file -->
   <img src="/website/assets/img/kaggle-bengali/midway-blog/densenet-structural-table.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Densenet structure look-up table" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

  <figcaption>Densenet structural reference table</figcaption>
</div>

<h2 id="model-construction">Model construction</h2>

<p>The model is constructed using the <code class="language-plaintext highlighter-rouge">Densenet121</code> template implemented in <code class="language-plaintext highlighter-rouge">TensorFlow</code>. The model was built with deep learning API <a href="https://keras.io/about/" rel="external nofollow noopener" target="_blank">Keras</a>. The code to construct the model is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_densenet</span><span class="p">(</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="n">densenet</span> <span class="o">=</span> <span class="nc">DenseNet121</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="sh">'</span><span class="s">imagenet</span><span class="sh">'</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="nb">input</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="nf">densenet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="nc">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># multi output
</span>    <span class="n">grapheme_root</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">168</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">root</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">vowel_diacritic</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">vowel</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">consonant_diacritic</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">consonant</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">grapheme_root</span><span class="p">,</span> <span class="n">vowel_diacritic</span><span class="p">,</span> <span class="n">consonant_diacritic</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">build_densenet</span><span class="p">(</span><span class="n">SIZE</span><span class="o">=</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div></div>

<p>Here we use a dropout rate of 0.3.
Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.</p>

<p>Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.</p>

<p><code class="language-plaintext highlighter-rouge">relu</code> is short for rectified linear unit, which is an activation function defined as \(max(0,x)\). The rectifier activation function is used to introduce non-linearity into the neural networks.</p>

<p>A summary of the model can be seen if you run <code class="language-plaintext highlighter-rouge">model.summary()</code>; it should look like something below.</p>

<div class="img-div" style="text-align:center">
  <figure>

  <picture>
    

    <!-- Fallback to the original file -->
   <img src="/website/assets/img/kaggle-bengali/midway-blog/model-summary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Summary of densenet model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

  <figcaption>Summary of the Densenet121 model built using Keras</figcaption>
</div>

<h2 id="optimizer-and-learning-schedule">Optimizer and Learning schedule</h2>

<p>We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the observed labels and the predicted ones. We use a specific form for categorical classifications of multiple classes termed <code class="language-plaintext highlighter-rouge">categorical_crossentropy</code>.</p>

<p><code class="language-plaintext highlighter-rouge">Adam</code> optimizer realizes the benefits of both <code class="language-plaintext highlighter-rouge">AdaGrad</code> and <code class="language-plaintext highlighter-rouge">RMSProp</code>. Instead of adapting the parameter learning rates based on the average first moment (the mean) as in <code class="language-plaintext highlighter-rouge">RMSProp</code>, <code class="language-plaintext highlighter-rouge">Adam</code> also makes use of the average of the second moments of the gradients (the uncentered variance). Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters <code class="language-plaintext highlighter-rouge">beta1</code> and <code class="language-plaintext highlighter-rouge">beta2</code> control the decay rates of these moving averages.</p>

<p>The metric function <code class="language-plaintext highlighter-rouge">accuracy</code> is used is to evaluate the performance of our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation).</p>

<p>Code for the setting the optimizer and fixed learning rate is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">root</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="sh">'</span><span class="s">vowel</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sh">'</span><span class="s">consonant</span><span class="sh">'</span><span class="p">:</span><span class="mf">0.3</span><span class="p">}</span>
<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="nc">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.00016</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss_weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>
<p>In order to make the optimizer converge faster and closest to the global minimum of the loss function, I used an annealing method of the learning rate (LR).
The LR is the step by which the optimizer walks through the ‘loss landscape’. The higher LR, the bigger are the steps and the quicker is the convergence. However, the sampling is very poor with a high LR and the optimizer could probably fall into a local minimum.
It’s better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Learning rate will be half after 3 epochs if accuracy is not increased
</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">root</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">vowel</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">consonant</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
 <span class="n">lr_scheduler</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s">_accuracy</span><span class="sh">'</span><span class="p">,</span>
                     <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                     <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">))</span>
<span class="c1"># Callback : Save best model
</span><span class="n">cp</span> <span class="o">=</span> <span class="nc">ModelCheckpoint</span><span class="p">(</span><span class="sh">'</span><span class="s">saved_models/densenet121_128x128_1-rr.h5</span><span class="sh">'</span><span class="p">,</span>
<span class="n">monitor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">val_root_accuracy</span><span class="sh">'</span><span class="p">,</span><span class="n">save_best_only</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="n">save_weights_only</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span><span class="n">mode</span> <span class="o">=</span> <span class="sh">'</span><span class="s">auto</span><span class="sh">'</span><span class="p">,</span><span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>
<p><strong><code class="language-plaintext highlighter-rouge">ModelCheckPoint</code></strong> is used to save the whole model or just the weights if our model improves by the criteria of improvement defined.</p>

<h2 id="data-augmentation">Data augmentation</h2>

<p>In order to avoid the overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occurring when someone is writing a digit.</p>

<p>By applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.</p>

<p>For the data augmentation strategies, I chose to:</p>

<ul>
  <li>Randomly rotate some training images by 8 degrees</li>
  <li>Randomly Zoom by 15% some training images</li>
  <li>Randomly shift images horizontally by 15% of the width</li>
  <li>Randomly shift images vertically by 15% of the height</li>
</ul>

<p>The improvement is critical:</p>

<ul>
  <li>Without data augmentation, I obtained an accuracy of 81.85%, 95.02%, and 94.95% for respective grapheme roots, vowel diacritics and consonant diacritics.</li>
  <li>With data augmentation, I achieved an accuracy of 90.07%, 96.71%, and 97.11%.</li>
</ul>

<p>Code for image augmentation is shown below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Data augmentation for creating more training data
</span><span class="n">datagen</span> <span class="o">=</span> <span class="nc">MultiOutputDataGenerator</span><span class="p">(</span>
    <span class="n">featurewise_center</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># set input mean to 0 over the dataset
</span>    <span class="n">samplewise_center</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># set each sample mean to 0
</span>    <span class="n">featurewise_std_normalization</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># divide inputs by std of the dataset
</span>    <span class="n">samplewise_std_normalization</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># divide each input by its std
</span>    <span class="n">zca_whitening</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># apply ZCA whitening
</span>    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># randomly rotate images in the range (degrees, 0 to 180)
</span>    <span class="n">zoom_range</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span> <span class="c1"># Randomly zoom image
</span>    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>  <span class="c1"># randomly shift images horizontally (fraction of total width)
</span>    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>  <span class="c1"># randomly shift images vertically (fraction of total height)
</span>    <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># randomly flip images
</span>    <span class="n">vertical_flip</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># randomly flip images
</span></code></pre></div></div>

<p>In the final blog, we will talk about the evaluation steps and methods to improve the model. Check the leaderboard results as well.</p>

    </div>
  </article>
</div>

          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Cheng  Zeng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a> theme.
Last updated: September 21, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/website/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/website/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/website/assets/js/no_defer.js"></script>
  <script defer src="/website/assets/js/common.js"></script>
  <script defer src="/website/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
